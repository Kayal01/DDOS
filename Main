import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Replace 'your_path_here' with the actual file path
file_path = 'Wednesday-workingHours.pcap_ISCX.csv'

# Read the dataset
df = pd.read_csv(file_path)

# Displaying the first few rows of the dataframe to understand its structure
print(df.head())
# Display basic information about the dataset
print(df.info())

# Visualize the distribution of classes
class_distribution = df.iloc[:, -1].value_counts()  # Adjust column index if necessary
plt.figure(figsize=(10,6))
class_distribution.plot(kind='bar', color='skyblue')
plt.title('Class Distribution')
plt.ylabel('Frequency')
plt.xlabel('Class')
plt.show()
df.isnull().sum()
for col in df.columns:
    # Check if the column is of a numeric data type (int or float)
    if df[col].dtype == 'float64' or df[col].dtype == 'int64':
        # Calculate the mean of the column
        col_mean = df[col].mean()
        # Replace NaN values with the mean
        df[col].fillna(col_mean, inplace=True)
from sklearn.preprocessing import LabelEncoder


# Initialize a LabelEncoder
labelencoder = LabelEncoder()

# Fit the label encoder and return encoded labels
df['Label'] = labelencoder.fit_transform(df['Label'])
print(df.head())

# Visualize the distribution of classes
class_distribution = df.iloc[:, -1].value_counts()  # Adjust column index if necessary
plt.figure(figsize=(10,6))
class_distribution.plot(kind='bar', color='skyblue')
plt.title('Class Distribution')
plt.ylabel('Frequency')
plt.xlabel('Class')
plt.show()
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder


# 1. Handle Missing Values: Impute or remove
df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infinities with NaN
df.dropna(inplace=True)  # Option to drop NaNs, or you could choose to impute them

# 2. Handle Extreme Values: Cap or transform
# (This step depends greatly on domain knowledge; consider capping values or transforming variables)

# 3. Feature Scaling: Standardize or Normalize features
scaler = StandardScaler()  # or MinMaxScaler or another appropriate scaler
X = df.drop('Label', axis=1)
X_scaled = scaler.fit_transform(X)

# 4. Encode Categorical Variables (if any, and if 'label' is categorical, encode it as well)
le = LabelEncoder()
y = le.fit_transform(df['Label'])



def percentile_capping(df, lower_percentile, upper_percentile):
    for col in df.select_dtypes(include=[np.number]):  # This ensures we only process numeric columns
        # Calculate percentiles
        lower_bound = df[col].quantile(lower_percentile)
        upper_bound = df[col].quantile(upper_percentile)

        # Cap the data
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    return df

# Apply the function with your chosen percentiles, e.g., 1st and 99th percentiles
df = percentile_capping(df, 0.10, 0.90)

# Separate the features (X) and the target (y)
X = df.drop('Label', axis=1)
y = df['Label']


# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize KNN classifier
knn = KNeighborsClassifier()

# Create a dictionary of all values we want to test for n_neighbors, weights, and p
param_grid = {'n_neighbors': [1,3,5],  # example range, adjust based on dataset
              'weights': ['uniform', 'distance'],
              'p': [1, 2]}

# Use grid search to test all values for n_neighbors, weights, and p
knn_gscv = GridSearchCV(knn, param_grid, cv=2)  # 5-fold cross-validation


# Fit the model to data
knn_gscv.fit(X_train, y_train)

# Check top performing n_neighbors value and the corresponding score
best_parameters = knn_gscv.best_params_
best_score = knn_gscv.best_score_

# Visualization or additional model evaluation steps here
# For example, you could visualize the results of the grid search

print("Best parameters:", best_parameters)
print("Best score:", best_score)

# Extracting results for visualization
results = pd.DataFrame(knn_gscv.cv_results_)

# Visualization for n_neighbors
plt.figure(figsize=(16, 6))
for weight in ['uniform', 'distance']:
    for p in [1, 2]:
        subset = results[(results.param_weights == weight) & (results.param_p == p)]
        
        plt.plot(subset['param_n_neighbors'], subset['mean_test_score'], 
                 label=f'weights={weight}, p={p}')
        
plt.title('Model accuracy for different n_neighbors')
plt.xlabel('Number of Neighbors')
plt.ylabel('Mean Test Score')
plt.legend()
plt.show()

# Visualization for weights parameter against accuracy
plt.figure(figsize=(16, 6))
for weight in results['param_weights'].unique():
    weight_subset = results[results['param_weights'] == weight]
    plt.plot(weight_subset['param_n_neighbors'], weight_subset['mean_test_score'], label=f'Weight: {weight}')

plt.title('Model accuracy for different weights')
plt.xlabel('Number of Neighbors (n_neighbors)')
plt.ylabel('Mean Test Score (Accuracy)')
plt.legend()
plt.show()

# Visualization for p parameter (distance metric) against accuracy
plt.figure(figsize=(16, 6))
for p in results['param_p'].unique():
    p_subset = results[results['param_p'] == p]
    plt.plot(p_subset['param_n_neighbors'], p_subset['mean_test_score'], label=f'p (Minkowski metric): {p}')

plt.title('Model accuracy for different values of p (Minkowski metric)')
plt.xlabel('Number of Neighbors (n_neighbors)')
plt.ylabel('Mean Test Score (Accuracy)')
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import pandas as pd
import matplotlib.pyplot as plt

# Initialize RandomForestClassifier
rfc = RandomForestClassifier()

# Create a dictionary of all values we want to test
param_grid = {
    'n_estimators': [10, 50, 100],  # number of trees in the forest
    'max_depth': [None, 10, 20, 30],  # maximum depth of the tree
    'min_samples_split': [2, 5, 10]  # minimum number of samples required to split an internal node
}

# Use grid search to test all values
rfc_gscv = GridSearchCV(rfc, param_grid, cv=2)  # 2-fold cross-validation
rfc_gscv.fit(X_train, y_train)

# Check top performing parameter set
best_parameters = rfc_gscv.best_params_
best_score = rfc_gscv.best_score_

print("Best parameters:", best_parameters)
print("Best score:", best_score)

# Extracting results for visualization
results = pd.DataFrame(rfc_gscv.cv_results_)

# Visualization for n_estimators
plt.figure(figsize=(16, 6))
for depth in results['param_max_depth'].unique():
    for split in results['param_min_samples_split'].unique():
        subset = results[(results.param_max_depth == depth) & (results.param_min_samples_split == split)]
        
        plt.plot(subset['param_n_estimators'], subset['mean_test_score'], 
                 label=f'max_depth={depth}, min_samples_split={split}')
        
plt.title('Model accuracy for different n_estimators (Number of Trees)')
plt.xlabel('Number of Trees (n_estimators)')
plt.ylabel('Mean Test Score')
plt.legend()
plt.show()

# Additional visualizations can be created for 'max_depth' and 'min_samples_split' similar to the above

from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt

# Assuming knn_gscv and rfc_gscv are your trained GridSearchCV models for KNN and Random Forest respectively
# And X_test, y_test are your test data

# Predict with the best models
knn_predictions = knn_gscv.best_estimator_.predict(X_test)
rfc_predictions = rfc_gscv.best_estimator_.predict(X_test)

# Calculate metrics
knn_accuracy = accuracy_score(y_test, knn_predictions)
rfc_accuracy = accuracy_score(y_test, rfc_predictions)

knn_f1 = f1_score(y_test, knn_predictions, average='weighted')
rfc_f1 = f1_score(y_test, rfc_predictions, average='weighted')

# Data for plotting
classifiers = ['KNN', 'Random Forest']
accuracies = [knn_accuracy, rfc_accuracy]
f1_scores = [knn_f1, rfc_f1]

# Plotting accuracy comparison
plt.figure(figsize=(10, 5))
plt.bar(classifiers, accuracies, color=['blue', 'green'])
plt.title('Accuracy Comparison')
plt.ylabel('Accuracy')
for i in range(len(classifiers)):
    plt.text(i, accuracies[i], round(accuracies[i], 2), ha = 'center')
plt.show()

# Plotting F1 score comparison
plt.figure(figsize=(10, 5))
plt.bar(classifiers, f1_scores, color=['orange', 'red'])
plt.title('F1 Score Comparison')
plt.ylabel('F1 Score')
for i in range(len(classifiers)):
    plt.text(i, f1_scores[i], round(f1_scores[i], 2), ha = 'center')
plt.show()

